#Twitterからツイートを取得してテキストマイニングを行うプログラム
install.packages("wordcloud")
install.packages("wordcloud2")
install.packages("RMeCab", repos = "https://rmecab.jp/R") 
install.packages("rtweet",dependencies=TRUE)
install.packages('tm')
install.packages("dplyr")
install.packages("purrr")
install.packages("stringr")
install.packages("syuzhet")



library(rtweet)
library(tm)
library(RMeCab)
library(dplyr)
library(purrr)
library(stringr)
library(wordcloud)
library(wordcloud2)
library(ggplot2)
library(syuzhet)


# 認証情報
CONSUMERKEY = ""
CONSUMERSECRET = ""
APPNAME = ""

twitter_token = create_token(
  app = APPNAME,
  consumer_key = CONSUMERKEY,
  consumer_secret = CONSUMERSECRET
)

# @MotokazuUdagawaのタイムライン取得
udagawa_home_timeline = get_timeline("MotokazuUdagawa", n = 1000, home = FALSE, token = twitter_token)

# テキスト情報を取得
udagawa_home_timeline_texts = udagawa_home_timeline$text

# 日本語のみ抽出
udagawa_home_timeline_texts_onlyJa = str_replace_all(udagawa_home_timeline_texts, "\\p{ASCII}", "")
udagawa_home_timeline_texts_onlyJa_shiftJis = udagawa_home_timeline_texts_onlyJa %>% iconv(from = "UTF-8", to = "CP932") %>% na.omit()

# 200件のツイートを1つのテキストとしてまとめる
udagawa_home_tweets_all = ""
for (i in 1:length((udagawa_home_timeline_texts_onlyJa_shiftJis))){
  udagawa_home_tweets_all = paste(udagawa_home_tweets_all, udagawa_home_timeline_texts_onlyJa_shiftJis[i], seq = "")
}

# あとで使えるように保存
write.table(udagawa_home_tweets_all, "udagawa_home_timeline_texts.txt")
write.table(udagawa_home_tweets_all, "udagawa_home_tweets_all200.txt")

# 先ほど保存したテキストファイルから形態素解析実行
docDF_udagawa = docDF("udagawa_home_tweets_all200.txt", type = 1)
# 名詞情報かつ非自立でないものを抽出
docDF_udagawa2 = docDF_udagawa %>% filter(POS1 %in% c("名詞"), POS2 != "非自立") 

# Word CLoud実行
wordcloud(docDF_udagawa2$TERM,docDF_udagawa2$udagawa_home_tweets_all200.txt, min.freq= 3,shape='diamond', scale=c(6,1), family ="JP1"
          , colors = brewer.pal(8,"Dark2"))

# 解析対象となるデータの読み込み
res <- RMeCabFreq("udagawa_home_timeline_texts.txt")

# 感動詞だけを取り出してデータフレームres_nounへ
res_noun <- res[res[,2]=="名詞",]

# 2回以上登場する名詞の数。res[,4]で"Freq"列を参照
nrow(res_noun <- res[res[,2]=="名詞" & res[,4] > 1,])

# res_nounをFreqで降順ソート
res_noun[rev(order(res_noun$Freq)),]

# 1列目と4列目を抜き出してデータフレームを作成する
res_noun2 <- data.frame(word=as.character(res_noun[,1]),
                        freq=res_noun[,4])

# 上位25位に絞り込む
res_noun2 <- subset(res_noun2, rank(-freq)<50)

# ggplotでグラフを描画する
ggplot(res_noun2, aes(x=reorder(word,freq), y=freq)) +
  geom_bar(stat = "identity", fill="orange") +
  theme_bw(base_size = 10, base_family = "HiraKakuProN-W3") +
  coord_flip() 

# Converting tweets to ASCII to trackle strange characters
tweets <- iconv(udagawa_home_timeline, from="UTF-8", to="ASCII", sub="")
# removing retweets, in case needed 
tweets <-gsub("(RT|via)((?:\\b\\w*@\\w+)+)","",tweets)
# removing mentions, in case needed
tweets <-gsub("@\\w+","",tweets)
ew_sentiment<-get_nrc_sentiment((tweets))
sentimentscores<-data.frame(colSums(ew_sentiment[,]))
names(sentimentscores) <- "Score"
sentimentscores <- cbind("sentiment"=rownames(sentimentscores),sentimentscores)
rownames(sentimentscores) <- NULL
ggplot(data=sentimentscores,aes(x=sentiment,y=Score))+
  geom_bar(aes(fill=sentiment),stat = "identity")+
  theme(legend.position="none")+
  xlab("Sentiments")+ylab("Scores")+
  ggtitle("Total sentiment based on scores")+
  theme_minimal()
